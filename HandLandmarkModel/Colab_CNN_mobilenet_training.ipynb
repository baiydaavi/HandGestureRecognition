{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport os\nfrom tqdm import tqdm\nfrom time import sleep\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2021-07-14T03:06:50.433666Z","iopub.execute_input":"2021-07-14T03:06:50.434072Z","iopub.status.idle":"2021-07-14T03:06:51.860473Z","shell.execute_reply.started":"2021-07-14T03:06:50.433988Z","shell.execute_reply":"2021-07-14T03:06:51.859596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DataLoader","metadata":{}},{"cell_type":"code","source":"import albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n    \ndata_dir = '../input/asl-alphabet/asl_alphabet_train/asl_alphabet_train'\ndef load_split_train_test(datadir, batch_size, valid_size = .2):\n         #No need to resize images since they are all 200 x 200 x 3\n    train_transforms = transforms.Compose([transforms.Resize((224, 224)),transforms.ColorJitter(0.9, 0.9, 0.9, 0.1), transforms.ToTensor(),transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n\n    test_transforms = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(),transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n\n    train_data = datasets.ImageFolder(datadir, transform=train_transforms)\n    test_data = datasets.ImageFolder(datadir, transform=test_transforms)\n\n    num_train = len(train_data)\n    indices = list(range(num_train))\n    split = int(np.floor(valid_size * num_train))\n    np.random.shuffle(indices)\n\n    train_idx, test_idx = indices[split:], indices[:split]\n    train_sampler = SubsetRandomSampler(train_idx)\n    test_sampler = SubsetRandomSampler(test_idx)\n\n    trainloader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n    testloader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n\n    return trainloader, testloader\n\nbatch_size = 32\ntrainloader, testloader = load_split_train_test(data_dir, batch_size, .18)\nprint(\"Train Size:\", len(trainloader) * batch_size, \", No of bacthes:\", len(trainloader))\nprint(\"Test Size:\", len(testloader) * batch_size, \", No of bacthes:\", len(testloader))\nprint(\"Classes:\", trainloader.dataset.classes)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"device:\",device)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T03:06:51.862015Z","iopub.execute_input":"2021-07-14T03:06:51.862392Z","iopub.status.idle":"2021-07-14T03:07:34.307636Z","shell.execute_reply.started":"2021-07-14T03:06:51.862352Z","shell.execute_reply":"2021-07-14T03:07:34.306825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class GaussianBlur(torch.nn.Module):\n    def __init__(self, kernel_size, std_dev):\n        self.kernel_size = kernel_size\n        self.std_dev = std_dev\n\n    def forward(self, img):\n        image = np.array(img)\n        image_blur = cv2.GaussianBlur(image, self.kernel_size, self.std_dev)\n        return Image.fromarray(image_blur)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T03:07:34.309483Z","iopub.execute_input":"2021-07-14T03:07:34.309838Z","iopub.status.idle":"2021-07-14T03:07:34.317424Z","shell.execute_reply.started":"2021-07-14T03:07:34.309802Z","shell.execute_reply":"2021-07-14T03:07:34.31661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"backbone = 'mobilenet_v2'\naddLayers = False\nif backbone == 'resnet50':\n    model = models.resnet50(pretrained=True)\n    for param in model.parameters():\n        param.requires_grad = False\n    if addLayers:\n        model.fc = nn.Sequential(nn.Linear(2048, 1024),\n          nn.ReLU(),\n          nn.Dropout(0.2),\n          nn.Linear(1024, len(trainloader.dataset.classes)),\n          nn.LogSoftmax(dim=1)\n          )\n    else:\n        model.fc = nn.Linear(2048, len(trainloader.dataset.classes))\nelif backbone == 'mobilenet_v2':\n    model = models.mobilenet_v2(pretrained=True)\n    for param in model.parameters():\n        param.requires_grad = False\n    if addLayers:\n        model.classifier = nn.Sequential(nn.Linear(1280, 1024),\n          nn.ReLU(),\n          nn.Dropout(0.2),\n          nn.Linear(1024, len(trainloader.dataset.classes)),\n          nn.LogSoftmax(dim=1)\n          )\n    else:\n        model.classifier = nn.Linear(1280, len(trainloader.dataset.classes))\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T03:07:34.31921Z","iopub.execute_input":"2021-07-14T03:07:34.319644Z","iopub.status.idle":"2021-07-14T03:07:34.828169Z","shell.execute_reply.started":"2021-07-14T03:07:34.31961Z","shell.execute_reply":"2021-07-14T03:07:34.827213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss and Optimizer","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#criterion = nn.NLLLoss()\nlearning_rate = 0.001\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.classifier.parameters(), lr=learning_rate)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T03:07:34.829583Z","iopub.execute_input":"2021-07-14T03:07:34.829949Z","iopub.status.idle":"2021-07-14T03:07:39.194073Z","shell.execute_reply.started":"2021-07-14T03:07:34.829895Z","shell.execute_reply":"2021-07-14T03:07:39.193158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"epochs = 6\nrunning_loss = 0\ntrain_losses, test_losses = [], []\nmin_val_loss = None\nname = ''","metadata":{"execution":{"iopub.status.busy":"2021-07-14T03:07:39.195391Z","iopub.execute_input":"2021-07-14T03:07:39.195717Z","iopub.status.idle":"2021-07-14T03:07:39.19982Z","shell.execute_reply.started":"2021-07-14T03:07:39.195682Z","shell.execute_reply":"2021-07-14T03:07:39.198922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(epochs):\n    with tqdm(trainloader, unit=\"batch\") as tepoch:\n        accuracy = 0\n        for i, data in enumerate(tepoch):\n            inputs, labels = data\n            tepoch.set_description(f\"Training Epoch {epoch + 1}\")\n            inputs, labels = inputs.to(device), labels.to(device)\n            size = labels.shape[0]\n            optimizer.zero_grad()\n            logps = model.forward(inputs)\n            # print(torch.argmax(logps, dim=1).shape, labels.shape)\n            loss = criterion(logps, labels)\n            train_acc = torch.sum(torch.argmax(logps, dim=1) == labels).item() / size\n            accuracy += train_acc\n            loss.backward()\n\n            optimizer.step()\n            running_loss += loss.item()\n\n            tepoch.set_postfix(loss=loss.item(), accuracy=100. * train_acc)\n            sleep(0.005)\n            if i == len(trainloader)-1:\n                accuracy = accuracy / len(trainloader)\n                tepoch.set_postfix(loss=running_loss/len(trainloader), accuracy=100. * accuracy)\n    test_loss = 0\n    accuracy = 0\n    model.eval()\n    with torch.no_grad():\n        with tqdm(testloader, unit=\"batch\") as tepoch:\n            for i, data in enumerate(tepoch):\n                (inputs, labels) = data\n                tepoch.set_description(f\"Testing Epoch {epoch + 1}\")\n                inputs, labels = inputs.to(device), labels.to(device)\n                size = labels.shape[0]\n                logps = model.forward(inputs)\n                batch_loss = criterion(logps, labels)\n                test_loss += batch_loss.item()\n\n                test_acc = torch.sum(torch.argmax(logps, dim=1) == labels).item() / size\n                accuracy += test_acc\n                tepoch.set_postfix(loss=batch_loss.item(), accuracy=100. * test_acc)\n                # tepoch.set_postfix(loss=batch_loss.item())\n                sleep(0.005)\n                if i == len(testloader)-1:\n                        accuracy = accuracy / len(testloader)\n                        tepoch.set_postfix(loss=test_loss/len(testloader), accuracy=100. * accuracy)\n\n    val_loss = test_loss/len(testloader)\n    if min_val_loss is None:\n        min_val_loss = val_loss\n        name = 'sl_recognition_{}_{}_{}.pth'.format(str(epoch + 1), str(round(val_loss, 3)), str(round(accuracy, 3)))\n        torch.save(model, name)\n    elif min_val_loss > val_loss:\n        min_val_loss = val_loss\n        name = 'sl_recognition_{}_{}_{}.pth'.format(str(epoch + 1), str(round(val_loss, 3)), str(round(accuracy, 3)))\n        torch.save(model, name)\n\n    running_loss = 0\n    model.train()\ntorch.save(model, 'final_sl.pth')","metadata":{"execution":{"iopub.status.busy":"2021-07-14T03:07:39.201281Z","iopub.execute_input":"2021-07-14T03:07:39.201825Z","iopub.status.idle":"2021-07-14T04:32:31.444696Z","shell.execute_reply.started":"2021-07-14T03:07:39.20179Z","shell.execute_reply":"2021-07-14T04:32:31.443872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate Classes.txt","metadata":{}},{"cell_type":"code","source":"with open('classes.txt', 'w') as f:\n    for clas in trainloader.dataset.classes:\n        f.write(clas+'\\n')","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:32:31.447152Z","iopub.execute_input":"2021-07-14T04:32:31.44756Z","iopub.status.idle":"2021-07-14T04:32:31.452965Z","shell.execute_reply.started":"2021-07-14T04:32:31.447519Z","shell.execute_reply":"2021-07-14T04:32:31.451963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Convert to onnx","metadata":{}},{"cell_type":"code","source":"name =  '../input/modelm/sl_recognition_20_0.185_0.94.pth'\n\nonnx_model_path = \"sl.onnx\"\nmodel = torch.load(name)\nmodel.to(\"cpu\")\nmodel.eval()\ndummy_input = torch.randn(1, 3, 224, 224)\ntorch.onnx.export(model, dummy_input, onnx_model_path, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:32:31.454866Z","iopub.execute_input":"2021-07-14T04:32:31.455286Z","iopub.status.idle":"2021-07-14T04:32:36.807619Z","shell.execute_reply.started":"2021-07-14T04:32:31.455248Z","shell.execute_reply":"2021-07-14T04:32:36.806858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run Inference with opencv","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:32:36.809806Z","iopub.execute_input":"2021-07-14T04:32:36.810327Z","iopub.status.idle":"2021-07-14T04:32:36.814507Z","shell.execute_reply.started":"2021-07-14T04:32:36.810286Z","shell.execute_reply":"2021-07-14T04:32:36.813528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decrease_brightness(img, value=30):\n    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n    h, s, v = cv2.split(hsv)\n\n    lim = 255 - value\n    v[v > lim] = 255\n    v[v <= lim] -= value\n\n    final_hsv = cv2.merge((h, s, v))\n    img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n    return img","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:32:36.815794Z","iopub.execute_input":"2021-07-14T04:32:36.816224Z","iopub.status.idle":"2021-07-14T04:32:36.829053Z","shell.execute_reply.started":"2021-07-14T04:32:36.816188Z","shell.execute_reply":"2021-07-14T04:32:36.828183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 20))\nimageDir = '../input/ankutest2'\nnet =  cv2.dnn.readNetFromONNX(onnx_model_path) \nwith open('classes.txt', 'r') as f:\n    classes = f.read().split('\\n')\nfor i, image_name in enumerate(os.listdir(imageDir)):\n    image = cv2.imread(os.path.join(imageDir, image_name))\n    image = image[5:-5, 5:-5]\n    image = decrease_brightness(image, value=25)\n    blob = cv2.dnn.blobFromImage(image, 1.0 / 255, (224, 224),(0, 0, 0), swapRB=True, crop=False)\n    net.setInput(blob)\n    preds = net.forward()\n    biggest_pred_index = np.array(preds)[0].argmax()\n    ax = plt.subplot(6, 5, i + 1)\n    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    plt.title(\"predicted: {}, True: {}\".format(classes[biggest_pred_index], image_name.split('_test.jpg')[0]))\n    plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:32:36.830091Z","iopub.execute_input":"2021-07-14T04:32:36.83055Z","iopub.status.idle":"2021-07-14T04:32:43.724246Z","shell.execute_reply.started":"2021-07-14T04:32:36.830516Z","shell.execute_reply":"2021-07-14T04:32:43.723318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 20))\nimageDir = '../input/asl-alphabet/asl_alphabet_test/asl_alphabet_test'\nnet =  cv2.dnn.readNetFromONNX(onnx_model_path) \nwith open('classes.txt', 'r') as f:\n    classes = f.read().split('\\n')\nfor i, image_name in enumerate(os.listdir(imageDir)):\n    image = cv2.imread(os.path.join(imageDir, image_name))\n#     image = image[2:2+275, 2:2+275]\n#     image = image[5:-5, 5:-5]\n    blob = cv2.dnn.blobFromImage(image, 1.0 / 255, (224, 224),(0, 0, 0), swapRB=False, crop=False)\n    net.setInput(blob)\n    preds = net.forward()\n    biggest_pred_index = np.array(preds)[0].argmax()\n    ax = plt.subplot(6, 5, i + 1)\n    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    plt.title(\"predicted: {}, True: {}\".format(classes[biggest_pred_index], image_name.split('_test.jpg')[0]))\n    plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:32:43.72542Z","iopub.execute_input":"2021-07-14T04:32:43.725724Z","iopub.status.idle":"2021-07-14T04:32:51.330901Z","shell.execute_reply.started":"2021-07-14T04:32:43.725694Z","shell.execute_reply":"2021-07-14T04:32:51.329841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 20))\nimageDir = '../input/ankutest2'\nnet =  cv2.dnn.readNetFromONNX(onnx_model_path) \nwith open('classes.txt', 'r') as f:\n    classes = f.read().split('\\n')\nfor i, image_name in enumerate(os.listdir(imageDir)):\n    image = cv2.imread(os.path.join(imageDir, image_name))\n#     image = image[2:2+275, 2:2+275]\n    image = image[5:-5, 5:-5]\n    blob = cv2.dnn.blobFromImage(image, 1.0 / 255, (224, 224),(0, 0, 0), swapRB=False, crop=False)\n    net.setInput(blob)\n    preds = net.forward()\n    biggest_pred_index = np.array(preds)[0].argmax()\n    ax = plt.subplot(6, 5, i + 1)\n    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    plt.title(\"predicted: {}, True: {}\".format(classes[biggest_pred_index], image_name.split('_test.jpg')[0]))\n    plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2021-07-14T04:32:51.332509Z","iopub.execute_input":"2021-07-14T04:32:51.3329Z","iopub.status.idle":"2021-07-14T04:32:58.387392Z","shell.execute_reply.started":"2021-07-14T04:32:51.332863Z","shell.execute_reply":"2021-07-14T04:32:58.38664Z"},"trusted":true},"execution_count":null,"outputs":[]}]}