{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DNN_landmarks_train.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1X50fa4y7xi3Nd8kkYAMhGcKZrt4WTTXI","authorship_tag":"ABX9TyNCW99tw+QNwD8rr4Kz72vm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BkLjFu-JUj8h","executionInfo":{"status":"ok","timestamp":1624041330335,"user_tz":420,"elapsed":577,"user":{"displayName":"Avinash","photoUrl":"","userId":"03822310646586674578"}},"outputId":"ab8b3f05-50cf-4c65-e8cb-b21a57170f3c"},"source":["cd drive/MyDrive/github/HandGestureRecognition/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/github/HandGestureRecognition\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vwkqgwzm00XH"},"source":["from os import path\n","\n","import pandas as pd\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import torch.nn.functional as F\n","import torch.utils.tensorboard as tb\n","from torch import optim\n","\n","from utils import ConfusionMatrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CkznEWCV-maJ"},"source":["## create the Dataset class\n","class HGMDataset(Dataset):\n","  def __init__(self, csvpath):\n","    df = pd.read_csv(csvpath)\n","    df = df.sample(frac=1).reset_index(drop=True)       \n","    self.x = df.iloc[:,4:].values\n","    self.y = df['label'].values.reshape(-1,1)\n","\n","  def __len__(self):\n","    return len(self.y)\n","\n","  def __getitem__(self, idx):\n","    x_val  = torch.Tensor(self.x[idx])\n","    y_val  = torch.Tensor(self.y[idx])\n","    y_val  = y_val.type(torch.LongTensor)\n","    return { 'data': x_val,\n","            'target': y_val\n","            }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YITPgwI1SEc8"},"source":["## create the Dataset object\n","HGM_data = HGMDataset('asl_alphabet_train/labels.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZoMZmBk0WquR"},"source":["## create training and validation split \n","split = int(0.8 * len(HGM_data))\n","index_list = list(range(len(HGM_data)))\n","train_idx, valid_idx = index_list[:split], index_list[split:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zHt2sMFKWz4d"},"source":["## create sampler objects using SubsetRandomSampler\n","tr_sampler = SubsetRandomSampler(train_idx)\n","val_sampler = SubsetRandomSampler(valid_idx)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gcBWdn1bW4-V"},"source":["## create iterator objects for train and valid datasets\n","trainloader = DataLoader(HGM_data, batch_size=256, sampler=tr_sampler)\n","validloader = DataLoader(HGM_data, batch_size=256, sampler=val_sampler)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bj6Sp0aNXAOT"},"source":["## create the DNN model\n","class Model(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.hidden1 = nn.Linear(42, 256)\n","    self.hidden2 = nn.Linear(256, 128)\n","    self.hidden3 = nn.Linear(128, 64)\n","    self.output = nn.Linear(64, 28)\n","    self.dropout1 = nn.Dropout(0.25)\n","    self.dropout2 = nn.Dropout(0.5)\n","\n","  def forward(self, x):\n","    x = self.hidden1(x)\n","    x = F.relu(x)\n","    x = self.dropout2(x)\n","    x = self.hidden2(x)\n","    x = F.relu(x)\n","    x = self.dropout2(x)\n","    x = self.hidden3(x)\n","    x = F.relu(x)\n","    x = self.dropout1(x)\n","    x = self.output(x)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8YR0tcZMfJzz"},"source":["## initialize the model\n","model = Model()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"al6m8s48Z4Kd"},"source":["## choose loss function and optimizer \n","loss_function = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay= 1e-6, momentum = 0.9, nesterov = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6XlqBBeNaYuu"},"source":["## train the model\n","epochs = 2000\n","\n","for epoch in range(1, epochs+1):\n","\n","    train_loss, valid_loss = [], []\n","    train_confusionMatrix = ConfusionMatrix()\n","    valid_confusionMatrix = ConfusionMatrix()\n","\n","    ## training part \n","    model.train()\n","\n","    for _, batch in enumerate(trainloader):\n","        \n","        data, target = batch['data'], batch['target']\n","\n","        optimizer.zero_grad()\n","\n","        ## 1. forward propagation\n","        output = model(data)\n","\n","        train_confusionMatrix.add(output.argmax(1), target.squeeze(1))\n","\n","        ## 2. loss calculation\n","        loss = loss_function(output, target.squeeze(1))\n","\n","        ## 3. backward propagation\n","        loss.backward()\n","\n","        ## 4. weight optimization\n","        optimizer.step()\n","\n","        ## 5. log train loss   \n","        train_loss.append(loss.item())\n","        \n","    ## validation part \n","    model.eval()\n","\n","    for _, batch in enumerate(validloader):\n","\n","        data, target = batch['data'], batch['target']\n","        output = model(data)\n","        valid_confusionMatrix.add(output.argmax(1), target.squeeze(1))\n","        loss = loss_function(output, target.squeeze(1))\n","        valid_loss.append(loss.item())\n","\n","    \n","    print (\"Epoch:\", epoch, \"Training Loss: \", np.mean(train_loss),\n","           \"Training Acc: \", train_confusionMatrix.global_accuracy, \n","           \"Valid Loss: \", np.mean(valid_loss),\n","           \"Valid Acc: \", valid_confusionMatrix.global_accuracy,\n","           )\n","    \n","    # if epoch%100 == 0 and epoch != 0:\n","    #     torch.save(model.state_dict(), f'trained_models/DNN_landmarks_model_{epoch}.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Trqd8nzubqKq"},"source":["## save model\n","torch.save(model.state_dict(), f'trained_models/DNN_landmarks_model.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RnnzYGyca0vp","executionInfo":{"status":"ok","timestamp":1622932742196,"user_tz":420,"elapsed":141,"user":{"displayName":"Avinash","photoUrl":"","userId":"03822310646586674578"}},"outputId":"530dea04-c23b-4b7e-80a8-68b8394777dd"},"source":["## check model prediction\n","dataiter = iter(validloader)\n","batch = dataiter.next()\n","data, labels = batch['data'], batch['target']\n","output = model(data)\n","\n","_, preds_tensor = torch.max(output, 1)\n","preds = np.squeeze(preds_tensor.numpy())\n","\n","print (\"Actual:\", labels.numpy().reshape(-1)[:10])\n","print (\"Predicted:\", preds[:10])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Actual: [ 8 25 19  9 24 25 18 20 25  2]\n","Predicted: [ 8 25 19  9 24 25 18 20 25  2]\n","[ True  True  True  True  True  True  True  True  True  True]\n"],"name":"stdout"}]}]}