{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport os\nfrom tqdm import tqdm\nfrom time import sleep\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2021-06-05T11:40:09.213611Z","iopub.execute_input":"2021-06-05T11:40:09.213956Z","iopub.status.idle":"2021-06-05T11:40:09.218827Z","shell.execute_reply.started":"2021-06-05T11:40:09.213923Z","shell.execute_reply":"2021-06-05T11:40:09.218005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DataLoader","metadata":{}},{"cell_type":"code","source":"data_dir = '../input/asl-alphabet/asl_alphabet_train/asl_alphabet_train'\ndef load_split_train_test(datadir, batch_size, valid_size = .2):\n    train_transforms = transforms.Compose([transforms.Resize((224, 244)), transforms.ToTensor(),])\n    test_transforms = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(),])\n\n    train_data = datasets.ImageFolder(datadir, transform=train_transforms)\n    test_data = datasets.ImageFolder(datadir, transform=test_transforms)\n\n    num_train = len(train_data)\n    indices = list(range(num_train))\n    split = int(np.floor(valid_size * num_train))\n    np.random.shuffle(indices)\n\n    train_idx, test_idx = indices[split:], indices[:split]\n    train_sampler = SubsetRandomSampler(train_idx)\n    test_sampler = SubsetRandomSampler(test_idx)\n\n    trainloader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n    testloader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n\n    return trainloader, testloader\n\nbatch_size = 32\ntrainloader, testloader = load_split_train_test(data_dir, batch_size, .18)\nprint(\"Train Size:\", len(trainloader) * batch_size, \", No of bacthes:\", len(trainloader))\nprint(\"Test Size:\", len(testloader) * batch_size, \", No of bacthes:\", len(testloader))\nprint(\"Classes:\", trainloader.dataset.classes)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"device:\",device)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T11:40:13.778227Z","iopub.execute_input":"2021-06-05T11:40:13.778555Z","iopub.status.idle":"2021-06-05T11:41:25.175561Z","shell.execute_reply.started":"2021-06-05T11:40:13.778524Z","shell.execute_reply":"2021-06-05T11:41:25.174748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Building","metadata":{}},{"cell_type":"code","source":"import torch\nclass CNNClassifier(torch.nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        self.master = torch.nn.Sequential(torch.nn.Conv2d(3, 16, kernel_size=7, padding=3, stride=2),\n                                          torch.nn.ReLU(),\n                                          torch.nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n        torch.nn.init.xavier_normal_(self.master[0].weight)\n\n        self.block1 = torch.nn.Sequential(torch.nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1),\n                                          torch.nn.BatchNorm2d(16),\n                                          torch.nn.ReLU(),\n                                          torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n                                          torch.nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True),\n                                          torch.nn.ReLU())\n\n        self.block2 = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1),\n            torch.nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n            torch.nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True),\n            torch.nn.ReLU())\n\n        self.block3 = torch.nn.Sequential(\n            torch.nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n            torch.nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n            torch.nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True),\n            torch.nn.ReLU())\n\n        self.relu = torch.nn.ReLU()\n        self.maxpool = torch.nn.MaxPool2d(kernel_size=3, padding=1, stride=2)\n        self.avgpool = torch.nn.AdaptiveAvgPool2d(output_size=(1, 1))\n\n        self.downsample1 = torch.nn.Sequential(torch.nn.Conv2d(16, 32, kernel_size=1),\n                                               torch.nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True),\n                                               torch.nn.ReLU())\n        self.downsample2 = torch.nn.Sequential(torch.nn.Conv2d(32, 64, kernel_size=1),\n                                               torch.nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True),\n                                               torch.nn.ReLU())\n        self.downsample3 = torch.nn.Sequential(torch.nn.Conv2d(64, 128, kernel_size=1),\n                                               torch.nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True),\n                                               torch.nn.ReLU())\n\n        self.classifier = torch.nn.Sequential(\n            torch.nn.Dropout(p=0.5),\n            torch.nn.Linear(128, 256),\n            torch.nn.BatchNorm1d(256),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.Dropout(p=0.5),\n            torch.nn.Linear(256, 256),\n            torch.nn.BatchNorm1d(256),\n            torch.nn.ReLU(inplace=True),\n            torch.nn.Dropout(p=0.5),\n            torch.nn.Linear(256, len(trainloader.dataset.classes)),\n            nn.LogSoftmax(dim=1)\n        )\n\n    def forward(self, x):\n        # print(x.shape)\n        # normalize image\n\n        mu = torch.mean(torch.mean(x, dim=2), dim=2).unsqueeze(-1).unsqueeze(-1)\n        sigma = torch.sqrt(torch.mean((x - mu) ** 2)) + 1e-8\n        x -= mu\n        x /= 4 * sigma\n\n        # print(\"image\", identity.shape)\n        res1 = self.master(x)\n\n        res2 = self.block1(res1)\n        res2 = res2 + self.downsample1(res1)\n        res2 = self.maxpool(res2)\n\n        res3 = self.block2(res2)\n        res3 = res3 + self.downsample2(res2)\n        res3 = self.maxpool(res3)\n\n        res4 = self.block3(res3)\n        # print(\"4 \", res4.shape ,self.downsample3(res3).shape )\n        res4 = res4 + self.downsample3(res3)\n\n        res = self.maxpool(res4)\n        # print(\"final shape : \", res.shape)\n        res = res.mean(dim=[2, 3])\n        res = self.classifier(res)\n        return res\n","metadata":{"execution":{"iopub.status.busy":"2021-06-05T11:41:29.046393Z","iopub.execute_input":"2021-06-05T11:41:29.046717Z","iopub.status.idle":"2021-06-05T11:41:29.07182Z","shell.execute_reply.started":"2021-06-05T11:41:29.046688Z","shell.execute_reply":"2021-06-05T11:41:29.069519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"backbone = 'mobilenet_v2'\naddLayers = False\nif backbone == 'resnet50':\n    model = models.resnet50(pretrained=True)\n    for param in model.parameters():\n        param.requires_grad = False\n    if addLayers:\n        model.fc = nn.Sequential(nn.Linear(2048, 1024),\n          nn.ReLU(),\n          nn.Dropout(0.2),\n          nn.Linear(1024, len(trainloader.dataset.classes)),\n          nn.LogSoftmax(dim=1)\n          )\n    else:\n        model.fc = nn.Linear(2048, len(trainloader.dataset.classes))\nelif backbone == 'mobilenet_v2':\n    model = models.mobilenet_v2(pretrained=True)\n    for param in model.parameters():\n        param.requires_grad = False\n    if addLayers:\n        model.classifier = nn.Sequential(nn.Linear(1280, 1024),\n          nn.ReLU(),\n          nn.Dropout(0.2),\n          nn.Linear(1024, len(trainloader.dataset.classes)),\n          nn.LogSoftmax(dim=1)\n          )\n    else:\n        model.classifier = nn.Linear(1280, len(trainloader.dataset.classes))\n# print(model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loss and Optimizer","metadata":{}},{"cell_type":"code","source":"#criterion = nn.NLLLoss()\nmodel = CNNClassifier()\nlearning_rate = 0.001\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, weight_decay=1e-5)\n\n# optimizer = optim.Adam(model.classifier.parameters(), lr=learning_rate)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T11:42:04.975341Z","iopub.execute_input":"2021-06-05T11:42:04.975651Z","iopub.status.idle":"2021-06-05T11:42:08.934816Z","shell.execute_reply.started":"2021-06-05T11:42:04.975623Z","shell.execute_reply":"2021-06-05T11:42:08.934053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"code","source":"epochs = 10\nrunning_loss = 0\ntrain_losses, test_losses = [], []\nmin_val_loss = None\nname = ''","metadata":{"execution":{"iopub.status.busy":"2021-06-05T12:09:58.18476Z","iopub.execute_input":"2021-06-05T12:09:58.185192Z","iopub.status.idle":"2021-06-05T12:09:58.194023Z","shell.execute_reply.started":"2021-06-05T12:09:58.185151Z","shell.execute_reply":"2021-06-05T12:09:58.193078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(epochs):\n    with tqdm(trainloader, unit=\"batch\") as tepoch:\n        accuracy = 0\n        for i, data in enumerate(tepoch):\n            inputs, labels = data\n            tepoch.set_description(f\"Training Epoch {epoch + 1}\")\n            inputs, labels = inputs.to(device), labels.to(device)\n            size = labels.shape[0]\n            optimizer.zero_grad()\n            logps = model.forward(inputs)\n            # print(torch.argmax(logps, dim=1).shape, labels.shape)\n            loss = criterion(logps, labels)\n            train_acc = torch.sum(torch.argmax(logps, dim=1) == labels).item() / size\n            accuracy += train_acc\n            loss.backward()\n\n            optimizer.step()\n            running_loss += loss.item()\n\n            tepoch.set_postfix(loss=loss.item(), accuracy=100. * train_acc)\n            sleep(0.005)\n            if i == len(trainloader)-1:\n                accuracy = accuracy / len(trainloader)\n                tepoch.set_postfix(loss=running_loss/len(trainloader), accuracy=100. * accuracy)\n    test_loss = 0\n    accuracy = 0\n    model.eval()\n    with torch.no_grad():\n        with tqdm(testloader, unit=\"batch\") as tepoch:\n            for i, data in enumerate(tepoch):\n                (inputs, labels) = data\n                tepoch.set_description(f\"Testing Epoch {epoch + 1}\")\n                inputs, labels = inputs.to(device), labels.to(device)\n                size = labels.shape[0]\n                logps = model.forward(inputs)\n                batch_loss = criterion(logps, labels)\n                test_loss += batch_loss.item()\n\n                test_acc = torch.sum(torch.argmax(logps, dim=1) == labels).item() / size\n                accuracy += test_acc\n                tepoch.set_postfix(loss=batch_loss.item(), accuracy=100. * test_acc)\n                # tepoch.set_postfix(loss=batch_loss.item())\n                sleep(0.005)\n                if i == len(testloader)-1:\n                        accuracy = accuracy / len(testloader)\n                        tepoch.set_postfix(loss=test_loss/len(testloader), accuracy=100. * accuracy)\n\n    val_loss = test_loss/len(testloader)\n    if min_val_loss is None:\n        min_val_loss = val_loss\n        name = 'sl_recognition_{}_{}_{}.pth'.format(str(epoch + 1), str(round(val_loss, 3)), str(round(accuracy, 3)))\n        torch.save(model, name)\n    elif min_val_loss > val_loss:\n        min_val_loss = val_loss\n        name = 'sl_recognition_{}_{}_{}.pth'.format(str(epoch + 1), str(round(val_loss, 3)), str(round(accuracy, 3)))\n        torch.save(model, name)\n\n    running_loss = 0\n    model.train()\ntorch.save(model, 'final_sl.pth')","metadata":{"execution":{"iopub.status.busy":"2021-06-05T12:10:00.117161Z","iopub.execute_input":"2021-06-05T12:10:00.117852Z","iopub.status.idle":"2021-06-05T13:11:10.298255Z","shell.execute_reply.started":"2021-06-05T12:10:00.11781Z","shell.execute_reply":"2021-06-05T13:11:10.297398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('classes.txt', 'w') as f:\n    for clas in trainloader.dataset.classes:\n        f.write(clas+'\\n')","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:11:56.345526Z","iopub.execute_input":"2021-06-05T13:11:56.345854Z","iopub.status.idle":"2021-06-05T13:11:56.350837Z","shell.execute_reply.started":"2021-06-05T13:11:56.345815Z","shell.execute_reply":"2021-06-05T13:11:56.350072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"onnx_model_path = \"sl.onnx\"\nmodel = torch.load(name)\nmodel.to(\"cpu\")\nmodel.eval()\ndummy_input = torch.randn(1, 3, 224, 224)\ntorch.onnx.export(model, dummy_input, onnx_model_path, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:12:07.658059Z","iopub.execute_input":"2021-06-05T13:12:07.658374Z","iopub.status.idle":"2021-06-05T13:12:08.084989Z","shell.execute_reply.started":"2021-06-05T13:12:07.658346Z","shell.execute_reply":"2021-06-05T13:12:08.083996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate Classes.txt","metadata":{}},{"cell_type":"code","source":"with open('classes.txt', 'w') as f:\n    for clas in trainloader.dataset.classes:\n        f.write(clas+'\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Convert to onnx","metadata":{}},{"cell_type":"code","source":"onnx_model_path = \"sl.onnx\"\nmodel = torch.load(name)\nmodel.to(\"cpu\")\nmodel.eval()\ndummy_input = torch.randn(1, 3, 224, 224)\ntorch.onnx.export(model, dummy_input, onnx_model_path, verbose=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run Inference with opencv","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:14:53.978269Z","iopub.execute_input":"2021-06-05T13:14:53.978622Z","iopub.status.idle":"2021-06-05T13:14:53.98406Z","shell.execute_reply.started":"2021-06-05T13:14:53.978589Z","shell.execute_reply":"2021-06-05T13:14:53.983097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(20, 20))\nimageDir = '../input/asl-alphabet/asl_alphabet_test/asl_alphabet_test'\nnet =  cv2.dnn.readNetFromONNX(onnx_model_path) \nwith open('classes.txt', 'r') as f:\n    classes = f.read().split('\\n')\nfor i, image_name in enumerate(os.listdir(imageDir)):\n    image = cv2.imread(os.path.join(imageDir, image_name))\n    blob = cv2.dnn.blobFromImage(image, 1.0 / 255, (224, 224),(0, 0, 0), swapRB=True, crop=False)\n    net.setInput(blob)\n    preds = net.forward()\n    biggest_pred_index = np.array(preds)[0].argmax()\n    ax = plt.subplot(6, 5, i + 1)\n    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n    plt.title(\"predicted: {}, True: {}\".format(classes[biggest_pred_index], image_name.split('_test.jpg')[0]))\n    plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2021-06-05T13:14:57.210479Z","iopub.execute_input":"2021-06-05T13:14:57.210793Z","iopub.status.idle":"2021-06-05T13:14:57.287292Z","shell.execute_reply.started":"2021-06-05T13:14:57.210763Z","shell.execute_reply":"2021-06-05T13:14:57.285818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}